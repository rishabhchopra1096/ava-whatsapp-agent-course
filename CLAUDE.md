# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Plan & Review

### Before starting work

- Always in plan mode to make a plan
- After get the plan, make sure you Write the plan to .claude/tasks/TASK_NAME.md.
- The plan should be a detailed implementation plan and the reasoning behind them, as well as tasks broken down.
- If the task require external knowledge or certain package, also research to get latest knowledge (Use Task tool for research)
- Don't over plan it, always think MVP.
- Once you write the plan, firstly ask me to review it. Do not continue until I approve the plan.

### While implementing

- You should update the plan as you work.
- After you complete tasks in the plan, you should update and append detailed descriptions of the changes you made, so following tasks can be easily hand over to other engineers.

## Project Overview

**Ava** is a multi-modal WhatsApp AI agent that processes text, voice, and images. Built as an educational course project demonstrating production-level AI application development using LangGraph workflows, Docker containerization, and cloud deployment.

## Development Commands

The project uses a Makefile for common operations. All commands should be run from the `ava-whatsapp-agent-course/` directory:

### Docker Operations

```bash
make ava-build    # Build Docker containers
make ava-run      # Start complete system (Qdrant + Chainlit + WhatsApp webhook)
make ava-stop     # Stop running containers
make ava-delete   # Clean up containers and memory directories
```

### Code Quality

```bash
make format-fix   # Auto-format code with Ruff
make lint-fix     # Fix linting issues
make format-check # Check code formatting
make lint-check   # Check for linting issues
```

### Manual Setup (Alternative to Docker)

```bash
uv venv .venv                    # Create virtual environment
source .venv/bin/activate        # Activate environment (macOS/Linux)
uv pip install -e .             # Install dependencies
cp .env.example .env             # Copy environment template
```

## Architecture

### LangGraph Workflow (`src/ai_companion/graph/`)

The core workflow follows this sequence:

1. **memory_extraction_node** - Extracts memories from user messages
2. **router_node** - Determines response type (text/image/audio)
3. **context_injection_node** - Injects contextual information
4. **memory_injection_node** - Injects relevant memories
5. **Response nodes** - conversation_node, image_node, or audio_node
6. **summarize_conversation_node** - Conditional summarization

Entry point: `src/ai_companion/graph/graph.py:graph` (defined in langgraph.json)

### Key Components

- **State Management**: `AICompanionState` in `src/ai_companion/graph/state.py`
- **Multi-modal Processing**:
  - Speech: `src/ai_companion/modules/speech/` (Whisper STT, ElevenLabs TTS)
  - Image: `src/ai_companion/modules/image/` (Vision models, FLUX generation)
- **Memory Systems**:
  - Short-term: SQLite checkpointing at `/tmp/memory.db` (Railway-compatible path)
  - Long-term: Qdrant Cloud vector database (`src/ai_companion/modules/memory/long_term/`)
- **Interfaces**:
  - Chainlit: Web chat at `src/ai_companion/interfaces/chainlit/app.py`
  - WhatsApp: FastAPI webhook at `src/ai_companion/interfaces/whatsapp/`

### Container Services

- **Qdrant Cloud**: Vector database (external service)
- **Chainlit**: Web interface (port 8000)
- **WhatsApp**: Webhook service (port 8080)

## Production Deployment Status

### Railway Deployment (COMPLETED âœ…)

**Current Status:** Successfully deployed to Railway
- **URL**: `https://ava-whatsapp-agent-course-production.up.railway.app`
- **Port**: 8080 (WhatsApp webhook)
- **Plan**: Railway Hobby ($5/month) for 10GB Docker image support
- **Build Time**: ~70-100 seconds
- **Auto-deploy**: Triggered by GitHub pushes to main branch

**Railway-Specific Configurations:**
- **Dockerfile**: Removed `VOLUME ["/app/data"]` instruction (banned by Railway)
- **Database Path**: Changed to `/tmp/memory.db` (Railway writable path)
- **Environment Variables**: All API keys configured in Railway dashboard
- **Networking**: Public URL automatically generated by Railway

### WhatsApp Business API Integration (ACTIVE âœ…)

**Current Status:** Fully configured and operational
- **Webhook URL**: `https://ava-whatsapp-agent-course-production.up.railway.app/whatsapp_response`
- **Verify Token**: `ava_rishabh_webhook_verify_123`
- **Phone Number ID**: `778411868682055` (current test number)
- **Access Token**: Fresh token generated (expires periodically)
- **Webhook Fields**: Subscribed to `messages`

**Test Number Configuration:**
- **Test Number**: `+15551389173` (90-day free messaging)
- **Recipient Allowlist**: Required for development mode
- **Message Types**: Text, image, and audio supported
- **API Version**: Auto-upgraded to v23.0 from v21.0

## Environment Configuration

Required `.env` file in `ava-whatsapp-agent-course/`:

### Core AI Services
- `GROQ_API_KEY` - For LLM (Llama 3.3) and STT (Whisper)
- `ELEVENLABS_API_KEY` & `ELEVENLABS_VOICE_ID` - For TTS
- `TOGETHER_API_KEY` - For image generation (FLUX models)

### Memory & Database
- `QDRANT_URL` - Qdrant Cloud URL (https://xxx.gcp.cloud.qdrant.io:6333)
- `QDRANT_API_KEY` - Qdrant Cloud API key

### WhatsApp Integration
- `WHATSAPP_PHONE_NUMBER_ID` - Meta's phone number reference ID
- `WHATSAPP_TOKEN` - Access token from Meta Developers (expires periodically)
- `WHATSAPP_VERIFY_TOKEN` - Webhook verification token

### Model Overrides
- `SMALL_TEXT_MODEL_NAME=llama3-8b-8192` - Required for structured output support

## Model Configuration & Compatibility

### Working Model Combinations

**Current Stable Configuration:**
- `TEXT_MODEL_NAME`: `llama-3.3-70b-versatile` (main conversation)
- `SMALL_TEXT_MODEL_NAME`: `llama3-8b-8192` (memory analysis - supports structured output)
- `ITT_MODEL_NAME`: `meta-llama/llama-4-scout-17b-16e-instruct` (vision - working model)
- `STT_MODEL_NAME`: `whisper-large-v3-turbo` (speech-to-text)
- `TTS_MODEL_NAME`: `eleven_flash_v2_5` (text-to-speech via ElevenLabs)
- `TTI_MODEL_NAME`: `black-forest-labs/FLUX.1-schnell-Free` (text-to-image)

### Known Issues & Fixes

**Structured Output Requirements:**
- Memory analysis requires models that support structured output (MemoryAnalysis format)
- âŒ `gemma2-9b-it` - Does NOT support structured output
- âœ… `llama3-8b-8192` - Supports structured output
- âœ… `llama-3.1-70b-versatile` - Supports structured output

**Vision Model Updates:**
- âŒ `llama-3.2-90b-vision-preview` - DEPRECATED (causes "model decommissioned" errors)
- âœ… `meta-llama/llama-4-scout-17b-16e-instruct` - Current working vision model

**Error Symptoms:**
```
Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.
```
Usually indicates model incompatibility with structured output requirements.

## Debugging & Error Logging

### Enhanced Logging System (IMPLEMENTED âœ…)

**Comprehensive Error Tracking:**
- **Environment Variable Validation**: Logs all required API keys at startup
- **WhatsApp API Error Logging**: Detailed HTTP response logging with status codes
- **LangGraph Workflow Errors**: Full Python tracebacks for workflow failures
- **Media Upload Debugging**: Comprehensive logging for file upload issues

**Log Locations:**
- **Railway Deploy Logs**: Real-time application logs with detailed error traces
- **Print Output**: Immediate visibility with ðŸ” and ðŸš¨ emoji indicators
- **Logger Output**: Standard Python logging for production debugging

### Common Error Patterns & Solutions

**WhatsApp Integration Errors:**

1. **Token Expiration (401 Unauthorized)**
   ```json
   {"error": {"message": "Session has expired on Friday, 18-Apr-25"}}
   ```
   **Solution:** Generate new access token in Meta Developers Console

2. **Recipient Not Allowed (400 Bad Request)**
   ```json
   {"error": {"message": "(#131030) Recipient phone number not in allowed list"}}
   ```
   **Solution:** Add phone numbers to recipient allowlist in Meta Developer Console

3. **API Version Upgrade Warning**
   ```
   "x-ad-api-version-warning": "The call has been auto-upgraded to v23.0 as v21.0 will be deprecated"
   ```
   **Status:** Working correctly (auto-upgrade accepted)

**Model Compatibility Errors:**
```bash
groq.BadRequestError: Failed to call a function. tool_use_failed
```
**Solution:** Ensure `SMALL_TEXT_MODEL_NAME=llama3-8b-8192` in environment variables

## Testing and Quality Assurance

### Development Testing
- **Linting**: Ruff is configured for code quality
- **Formatting**: Ruff handles code formatting
- **Testing**: Run `make lint-check` and `make format-check` before commits

### Production Testing
- **Webhook Testing**: Use Meta's webhook test data or Graph API Explorer
- **End-to-End**: Send actual WhatsApp messages to test number
- **Multi-modal**: Test text, image, and voice message processing
- **Error Monitoring**: Check Railway logs for production issues

## Key Dependencies

- **LangGraph 0.2.60+** - Workflow orchestration framework
- **Groq** - LLM and STT services
- **Together AI** - Image generation
- **ElevenLabs** - Text-to-speech
- **Qdrant Cloud** - Vector database for long-term memory
- **FastAPI** - WhatsApp webhook endpoints
- **Chainlit 1.3.2+** - Web chat interface
- **Python 3.12+** with `uv` package manager
- **Railway** - Cloud deployment platform

## Development Workflow

### Local Development
1. Ensure `.env` is configured with required API keys
2. Use `make ava-run` to start the development environment
3. Access web interface at http://localhost:8000
4. WhatsApp webhook available at http://localhost:8080
5. Run `make format-fix` and `make lint-fix` before commits
6. Use `make ava-delete` to clean up between sessions

### Production Deployment
1. **Push to GitHub** - Railway auto-deploys from main branch
2. **Monitor Build** - Check Railway build logs for Docker build success
3. **Check Deploy** - Monitor Railway deploy logs for startup issues
4. **Test Webhook** - Send test data to verify WhatsApp integration
5. **Update Environment** - Use Railway dashboard for API key updates

## Voice Calling Integration (IN PROGRESS ðŸ”§)

### Vapi Voice Calling Implementation Status

**Current Status:** Core implementation complete, debugging streaming response format
- **Technology**: Vapi for phone infrastructure + ElevenLabs for voice + Custom LLM endpoint
- **Trigger**: Users say "call me" in WhatsApp â†’ Ava initiates phone call via `voice_calling_node`
- **Context**: WhatsApp conversation context passed to voice call through `voice_context_manager`

### Architecture Overview

**Voice Calling Flow:**
1. **WhatsApp Trigger** (`webhook_endpoint.py`) â†’ Router detects "call me" intent
2. **Voice Calling Node** (`graph/nodes.py:voice_calling_node`) â†’ Initiates call via Vapi
3. **Vapi Client** (`interfaces/vapi/vapi_client.py`) â†’ Creates assistant + makes outbound call
4. **Custom LLM Endpoint** (`interfaces/vapi/vapi_endpoints.py`) â†’ Processes voice conversations
5. **LangGraph Integration** â†’ Same AI brain as WhatsApp, just different interface

### Implementation Details

**Key Files & Functions:**
- `src/ai_companion/graph/nodes.py:voice_calling_node()` - Call initiation trigger
- `src/ai_companion/interfaces/vapi/vapi_client.py:make_outbound_call()` - Vapi SDK integration
- `src/ai_companion/interfaces/vapi/vapi_endpoints.py:handle_voice_chat()` - Custom LLM endpoint
- `src/ai_companion/interfaces/vapi/voice_context_manager.py` - Context preparation

**Required Environment Variables:**
```env
# Vapi Configuration
VAPI_API_PRIVATE_KEY=priv_your_key_here  # From Vapi dashboard  
VAPI_PHONE_NUMBER_ID=uuid_here           # Your Vapi phone number ID
RAILWAY_URL=https://your-app.up.railway.app  # Your deployment URL

# Voice Assistant Setup
ELEVENLABS_API_KEY=sk_...                # For consistent voice across WhatsApp/voice
ELEVENLABS_VOICE_ID=uju3...              # Ava's voice ID
```

### Current Implementation Status

**âœ… Working Components:**
- Phone call initiation from WhatsApp "call me" messages
- Vapi assistant creation with correct parameters (fixed camelCase â†’ snake_case issues)
- Custom LLM endpoint receiving requests from Vapi
- LangGraph processing voice conversations (same brain as WhatsApp)
- OpenAI-compatible request/response format
- Context continuity between WhatsApp and voice

**ðŸ”§ Active Debugging:**
- **Streaming Response Format**: Vapi sends `stream: True` but expects Server-Sent Events (SSE) format
- **Issue**: We return complete JSON responses instead of streaming chunks
- **Solution**: Implemented `stream_response_chunks()` function for OpenAI-compatible streaming
- **Status**: Testing streaming response implementation

**ðŸ› Recent Fixes Applied:**
1. **Parameter Naming**: Fixed 18 camelCase â†’ snake_case parameter issues in Vapi SDK calls
2. **Invalid Parameters**: Removed unsupported assistant configuration parameters
3. **Voice Provider**: Changed `"elevenlabs"` â†’ `"11labs"` for Vapi compatibility
4. **Import Errors**: Fixed package name references from `vapi-python` to `vapi_server_sdk`
5. **Deprecation Warnings**: Updated `.dict()` â†’ `.model_dump()` for Pydantic v2

### Voice Calling Debug Process

**Debugging Timeline:**
1. **Initial Issue**: Calls connected but no speech output from Vapi
2. **Investigation**: Added comprehensive logging to trace request/response flow
3. **Discovery**: Vapi expects streaming responses when `stream: True` is set
4. **Root Cause**: Format mismatch - complete JSON vs streaming SSE chunks
5. **Solution**: Implemented OpenAI-compatible streaming response format

**Debug Logging Added:**
```python
# Request analysis
print(f"ðŸ“¨ FULL VAPI REQUEST: {request.model_dump()}")

# LangGraph processing
print(f"ðŸš€ CALLING LANGGRAPH WITH: {graph_input}")
print(f"ðŸ“¥ LANGGRAPH RESPONSE: {response}")

# Response formatting  
print(f"âœ… VOICE RESPONSE SENT: {ava_response}")
print(f"ðŸ” COMPLETE VAPI RESPONSE: {response_dict}")
```

### Technical Insights Learned

**Vapi Custom LLM Requirements:**
- Must support both streaming (`stream: True`) and complete response modes
- Streaming responses require Server-Sent Events (SSE) format with specific headers
- OpenAI compatibility means exact adherence to request/response schemas
- Python SDK uses snake_case parameters, not camelCase like JavaScript APIs

**Critical Implementation Details:**
```python
# Streaming Response Format Required:
{
  "Content-Type": "text/event-stream",
  "Cache-Control": "no-cache", 
  "Connection": "keep-alive"
}

# Each chunk format:
data: {"choices": [{"delta": {"content": "word "}}]}

# Termination:
data: [DONE]
```

### Testing Voice Calls

**Current Testing Process:**
1. Send "call me" to WhatsApp bot
2. Monitor Railway logs for:
   - Router decision: `ðŸ¤– ROUTER DECISION: voice_call`
   - Assistant creation: `âœ… Voice assistant created`
   - LangGraph processing: Voice conversation flow
   - Response format: Complete vs streaming detection
3. Test actual phone call connection and speech output

### Next Steps

**Immediate Priority:**
1. **Deploy streaming response fix** - Test if SSE format resolves speech output
2. **Verify end-to-end flow** - Ensure complete WhatsApp â†’ Voice â†’ Response cycle
3. **Optimize assistant reuse** - Consider using existing assistant ID vs creating new ones
4. **Add conversation persistence** - Maintain context across multiple voice interactions

**Future Enhancements:**
- Voice call â†’ WhatsApp summary integration
- Multi-turn conversation handling
- Voice call analytics and logging
- Integration with calendar/scheduling features

## Future Architecture Planning

### Enhanced Ava Vision (Personal Assistant Features)

**Planned Features:**
- âœ… Voice calling via Vapi (COMPLETED)
- Note-taking with Notion integration
- Todo list management
- Calendar event management
- Scheduled activities

### Database Architecture Evolution

**Current (Simple):**
- Qdrant Cloud: Vector database for conversation memories
- SQLite: Short-term conversation checkpointing

**Planned (Enhanced):**
- **Supabase-Only Architecture** (recommended):
  - PostgreSQL with pgvector for vector search
  - User management, todos, calendar, notes tables
  - Real-time subscriptions
  - Built-in authentication
  - Cost: ~$25/month vs current ~$5/month

**Migration Effort:**
- **Primary Changes:** `src/ai_companion/modules/memory/long_term/vector_store.py` (complete rewrite)
- **Configuration:** Update settings.py, .env, docker-compose.yml
- **Dependencies:** Replace qdrant-client with supabase + asyncpg
- **Time Estimate:** 1-2 days for experienced developer, 3-4 days learning

### LangGraph Enhancement Requirements

**Current:** Simple linear workflow (5-6 nodes)
**Future:** Complex multi-path workflow (15-20 nodes)

**New Nodes Needed:**
- Intent Detection (identify feature requests)
- Action Router (direct to workflows)
- External API Integration (Notion, Google Calendar, WAPI)
- Multi-step Workflow Management
- Confirmation/Validation Nodes

**Complexity Increase:** 3-4x current graph size

## Common Issues & Debugging

### Startup Problems

**"Missing required environment variables"**
- Check Railway environment variables dashboard
- Ensure all API keys are set correctly
- Verify variable names match settings.py requirements

**"Connection refused" to Qdrant**
- Verify Qdrant Cloud URL and API key in Railway environment
- Check Qdrant Cloud service status
- Ensure network connectivity from Railway to Qdrant

### WhatsApp Integration Issues

**"Access token expired"**
- Generate new token in Meta Developers Console
- Update `WHATSAPP_TOKEN` in Railway environment variables
- Tokens typically expire every 60-90 days

**"Recipient not in allowed list"**
- Add test phone numbers to allowlist in Meta Developer Console
- Development mode limits to 5 approved numbers
- Consider upgrading to production for unlimited recipients

**"Webhook verification failed"**
- Verify `WHATSAPP_VERIFY_TOKEN` matches Meta Developer Console setting
- Check webhook URL is accessible and returns correct challenge

### Railway Deployment Issues

**"Docker image too large"**
- Requires Railway Hobby plan ($5/month) for 10GB limit
- Current image ~6.5GB due to AI model dependencies

**"Build timeouts"**
- Normal build time: 70-100 seconds
- Monitor build logs for specific failures
- Check uv package resolution for dependency conflicts

### Performance Optimization

**Slow Responses:**
- Consider upgrading to faster models
- Check API rate limits on Groq/ElevenLabs/Together
- Monitor Railway resource usage

**High Costs:**
- Current monthly estimate: ~$30-40 (Railway $5 + Qdrant $25 + API usage)
- Use smaller models for non-critical tasks
- Monitor token usage across all AI services

## Pedagogical Documentation Standards

### Commenting Approach for Learning

**Target Audience:** Developers with 1-month coding bootcamp knowledge
- Knows basic Python/JavaScript, variables, functions, if/else, loops
- Does NOT know OOP, external libraries, web development, Docker, APIs

**Comment Requirements:**
1. **File Headers:** Explain purpose, problem solved, define technical terms
2. **Import Explanations:** Why each library is needed with real-world analogies
3. **Line-by-line Comments:** What, why, how, and system connections
4. **Grounded Analogies:** Must reference specific Ava components, not generic concepts

**Example Standards:**
```python
# ELEVENLABS CLIENT MANAGER - Creates and reuses connection to voice synthesis service
# This is like having one phone line to ElevenLabs instead of dialing a new connection
# every time Ava needs to analyze an image. Much more efficient.
@property
def client(self) -> ElevenLabs:
    # CHECK IF CLIENT ALREADY EXISTS (SINGLETON PATTERN)
    # self._client starts as None during initialization
    if self._client is None:
        # CREATE NEW ELEVENLABS CLIENT WITH API KEY
        # This is the expensive operation we only want to do once
        self._client = ElevenLabs(api_key=settings.ELEVENLABS_API_KEY)
    return self._client
```

## important-instruction-reminders
Do what has been asked; nothing more, nothing less.
NEVER create files unless they're absolutely necessary for achieving your goal.
ALWAYS prefer editing an existing file to creating a new one.
NEVER proactively create documentation files (*.md) or README files. Only create documentation files if explicitly requested by the User.